{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a49517-8f55-4efb-bde4-9246a9fb1718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:22:55.654975Z",
     "iopub.status.busy": "2022-09-14T05:22:55.654577Z",
     "iopub.status.idle": "2022-09-14T05:23:02.554327Z",
     "shell.execute_reply": "2022-09-14T05:23:02.554946Z"
    },
    "papermill": {
     "duration": 6.9142,
     "end_time": "2022-09-14T05:23:02.557486",
     "exception": false,
     "start_time": "2022-09-14T05:22:55.643286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard==2.9.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.9.1)\r\n",
      "Requirement already satisfied: tensorflow==2.9.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.9.1)\r\n",
      "Requirement already satisfied: tensorflow-io==0.27.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.27.0)\r\n",
      "Requirement already satisfied: tensorflow-hub==0.12.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.12.0)\r\n",
      "Requirement already satisfied: minio==7.1.11 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (7.1.11)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (2.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (0.4.6)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (3.19.5)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (1.8.1)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (1.48.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (0.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (1.23.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (65.3.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (1.2.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (1.35.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (0.37.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 1)) (2.26.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (21.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (1.14.1)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (1.12)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (3.7.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (1.1.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (0.2.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (1.1.2)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (0.4.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (2.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (4.3.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (1.16.0)\r\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (2.9.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (0.27.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.9.1->-r requirements.txt (line 2)) (14.0.6)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from minio==7.1.11->-r requirements.txt (line 5)) (2021.5.30)\r\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from minio==7.1.11->-r requirements.txt (line 5)) (1.26.7)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 1)) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 1)) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 1)) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1->-r requirements.txt (line 1)) (1.3.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.9.1->-r requirements.txt (line 1)) (4.12.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1->-r requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1->-r requirements.txt (line 1)) (3.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.9.1->-r requirements.txt (line 1)) (2.1.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->tensorflow==2.9.1->-r requirements.txt (line 2)) (2.4.7)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.9.1->-r requirements.txt (line 1)) (3.8.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 1)) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1->-r requirements.txt (line 1)) (3.2.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523aa8f1-9168-4c90-a7a6-e3868c6de7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:02.643363Z",
     "iopub.status.busy": "2022-09-14T05:23:02.642900Z",
     "iopub.status.idle": "2022-09-14T05:23:13.642627Z",
     "shell.execute_reply": "2022-09-14T05:23:13.642110Z"
    },
    "papermill": {
     "duration": 11.007114,
     "end_time": "2022-09-14T05:23:13.642758",
     "exception": false,
     "start_time": "2022-09-14T05:23:02.635644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minio import Minio, S3Error\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015c3f38-75a4-426c-827f-8f7890b4af98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:13.654307Z",
     "iopub.status.busy": "2022-09-14T05:23:13.653830Z",
     "iopub.status.idle": "2022-09-14T05:23:13.655711Z",
     "shell.execute_reply": "2022-09-14T05:23:13.655308Z"
    },
    "papermill": {
     "duration": 0.008871,
     "end_time": "2022-09-14T05:23:13.655803",
     "exception": false,
     "start_time": "2022-09-14T05:23:13.646932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config Paramters\n",
    "minio_address = \"minio.ns-1.svc.cluster.local\"\n",
    "minio_access_key = \"kubeflow\"\n",
    "minio_secret_key = \"kubeflow123\"\n",
    "datasets_bucket = \"datasets\"\n",
    "preprocessed_data_folder = \"preprocessed-data\"\n",
    "tf_record_file_size = 500\n",
    "output_text_path = \"preprocess.output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea05f58-082c-4183-b05f-357a9a4d2dcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:13.663983Z",
     "iopub.status.busy": "2022-09-14T05:23:13.663531Z",
     "iopub.status.idle": "2022-09-14T05:23:13.665264Z",
     "shell.execute_reply": "2022-09-14T05:23:13.664861Z"
    },
    "papermill": {
     "duration": 0.00671,
     "end_time": "2022-09-14T05:23:13.665350",
     "exception": false,
     "start_time": "2022-09-14T05:23:13.658640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "minioClient = Minio(minio_address,\n",
    "                    access_key=minio_access_key,\n",
    "                    secret_key=minio_secret_key,\n",
    "                    secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e71bee-a8f8-41bb-870b-4ef88ac2290f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:13.674930Z",
     "iopub.status.busy": "2022-09-14T05:23:13.674526Z",
     "iopub.status.idle": "2022-09-14T05:23:14.146030Z",
     "shell.execute_reply": "2022-09-14T05:23:14.146424Z"
    },
    "papermill": {
     "duration": 0.477798,
     "end_time": "2022-09-14T05:23:14.146590",
     "exception": false,
     "start_time": "2022-09-14T05:23:13.668792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "try:\n",
    "    minioClient.fget_object(\n",
    "        datasets_bucket,\n",
    "        \"aclImdb_v1.tar.gz\",\n",
    "        \"/tmp/dataset.tar.gz\")\n",
    "except S3Error as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f975b18-cd1b-4d21-bd62-63c5217ac752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:14.156844Z",
     "iopub.status.busy": "2022-09-14T05:23:14.156446Z",
     "iopub.status.idle": "2022-09-14T05:23:52.872686Z",
     "shell.execute_reply": "2022-09-14T05:23:52.873095Z"
    },
    "papermill": {
     "duration": 38.72497,
     "end_time": "2022-09-14T05:23:52.874965",
     "exception": false,
     "start_time": "2022-09-14T05:23:14.149995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "extract_folder = f\"/tmp/{datasets_bucket}/\"\n",
    "\n",
    "with tarfile.open(\"/tmp/dataset.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d40fb01-2be9-42ec-91f7-399b4a2201ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:52.898499Z",
     "iopub.status.busy": "2022-09-14T05:23:52.897989Z",
     "iopub.status.idle": "2022-09-14T05:23:57.027686Z",
     "shell.execute_reply": "2022-09-14T05:23:57.028066Z"
    },
    "papermill": {
     "duration": 4.143174,
     "end_time": "2022-09-14T05:23:57.028249",
     "exception": false,
     "start_time": "2022-09-14T05:23:52.885075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and structure the data\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "dirs_to_read = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "\n",
    "for dir_name in dirs_to_read:\n",
    "    parts = dir_name.split(\"/\")\n",
    "    dataset = parts[1]\n",
    "    label = parts[2]\n",
    "    for filename in os.listdir(os.path.join(extract_folder, dir_name)):\n",
    "        with open(os.path.join(extract_folder, dir_name, filename), \"r\") as f:\n",
    "            content = f.read()\n",
    "            if dataset == \"train\":\n",
    "                train.append({\n",
    "                    \"text\": content,\n",
    "                    \"label\": label\n",
    "                })\n",
    "            elif dataset == \"test\":\n",
    "                test.append({\n",
    "                    \"text\": content,\n",
    "                    \"label\": label\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad49912-40fa-4ca2-9a4f-1e595bf39f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:57.052733Z",
     "iopub.status.busy": "2022-09-14T05:23:57.052304Z",
     "iopub.status.idle": "2022-09-14T05:23:58.884903Z",
     "shell.execute_reply": "2022-09-14T05:23:58.885282Z"
    },
    "papermill": {
     "duration": 1.850901,
     "end_time": "2022-09-14T05:23:58.885455",
     "exception": false,
     "start_time": "2022-09-14T05:23:57.034554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since we encode the data using the Universal Sentence Encoder model let's download it\n",
    "try:\n",
    "    minioClient.fget_object(\n",
    "        datasets_bucket,\n",
    "        \"models/universal-sentence-encoder_4.tar.gz\",\n",
    "        \"/tmp/universal-sentence-encoder_4.tar.gz\")\n",
    "except S3Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784c2e3b-0a09-49f8-8d30-a416dea89cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:23:58.895445Z",
     "iopub.status.busy": "2022-09-14T05:23:58.895034Z",
     "iopub.status.idle": "2022-09-14T05:24:41.268715Z",
     "shell.execute_reply": "2022-09-14T05:24:41.271769Z"
    },
    "papermill": {
     "duration": 42.386531,
     "end_time": "2022-09-14T05:24:41.275316",
     "exception": false,
     "start_time": "2022-09-14T05:23:58.888785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "se_model_prefix = \"universal-sentence-encoder/4\"\n",
    "extract_folder = f\"/tmp/{se_model_prefix}/\"\n",
    "\n",
    "with tarfile.open(\"/tmp/universal-sentence-encoder_4.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_folder)\n",
    "embed = tf.saved_model.load(extract_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7310127d-6e01-4622-8c8c-67b11eefdae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:24:41.451158Z",
     "iopub.status.busy": "2022-09-14T05:24:41.445700Z",
     "iopub.status.idle": "2022-09-14T05:24:41.639140Z",
     "shell.execute_reply": "2022-09-14T05:24:41.638792Z"
    },
    "papermill": {
     "duration": 0.297268,
     "end_time": "2022-09-14T05:24:41.639250",
     "exception": false,
     "start_time": "2022-09-14T05:24:41.341982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _embedded_sentence_feature(value):\n",
    "    # convert tensor to list of float values\n",
    "    input = value.numpy().ravel().tolist()\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=input))\n",
    "\n",
    "def _label_feature(value):\n",
    "    # convert tensor to list of float values\n",
    "    input = value.numpy().ravel().tolist()\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=input))\n",
    "\n",
    "def encode_label(label):\n",
    "    if label == \"pos\":\n",
    "        return tf.constant([1, 0])\n",
    "    elif label == \"neg\":\n",
    "        return tf.constant([0, 1])\n",
    "\n",
    "def serialize_example(label, sentence_tensor):\n",
    "    feature = {\n",
    "        \"sentence\": _embedded_sentence_feature(sentence_tensor[0]),\n",
    "        \"label\": _label_feature(label),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto\n",
    "\n",
    "def process_examples(records, prefix=\"\"):\n",
    "    print(f\"Process examples for prefix: {prefix}\")\n",
    "    import timeit\n",
    "    starttime = timeit.default_timer()\n",
    "    total_training = len(records)\n",
    "    print(f\"Total of {total_training} elements\")\n",
    "    total_batches = math.floor(total_training / tf_record_file_size)\n",
    "    if total_training % tf_record_file_size != 0:\n",
    "        total_batches += 1\n",
    "    print(f\"Total of {total_batches} files of {tf_record_file_size} records - {timeit.default_timer() - starttime}\")\n",
    "\n",
    "    counter = 0\n",
    "    file_counter = 0\n",
    "    buffer = []\n",
    "    file_list = []\n",
    "    for i in range(total_training):\n",
    "        counter += 1\n",
    "        sentence_embedding = embed([records[i][\"text\"]])\n",
    "        label_encoded = encode_label(records[i][\"label\"])\n",
    "        record = serialize_example(label_encoded, sentence_embedding)\n",
    "        buffer.append(record)\n",
    "\n",
    "        if len(buffer) >= tf_record_file_size:\n",
    "            # save this buffer of examples as a file to MinIO\n",
    "            counter = 0\n",
    "            file_counter += 1\n",
    "            file_name = f\"{prefix}_file{file_counter}.tfrecord\"\n",
    "            with open(file_name, \"w+\") as f:\n",
    "                with tf.io.TFRecordWriter(f.name, options=\"GZIP\") as writer:\n",
    "                    for example in buffer:\n",
    "                        writer.write(example.SerializeToString())\n",
    "\n",
    "            try:\n",
    "                minioClient.fput_object(datasets_bucket, f\"{preprocessed_data_folder}/{file_name}\", file_name)\n",
    "            except S3Error as err:\n",
    "                print(err)\n",
    "            file_list.append(file_name)\n",
    "            os.remove(file_name)\n",
    "            buffer = []\n",
    "            print(f\"Done with batch {file_counter}/{total_batches} - {timeit.default_timer() - starttime}\")\n",
    "    print(\"\")\n",
    "    if len(buffer) > 0:\n",
    "        file_counter += 1\n",
    "        file_name = f\"file{file_counter}.tfrecord\"\n",
    "        with open(file_name, \"w+\") as f:\n",
    "            with tf.io.TFRecordWriter(f.name) as writer:\n",
    "                for example in buffer:\n",
    "                    writer.write(example.SerializeToString())\n",
    "        try:\n",
    "            minioClient.fput_object(datasets_bucket, f\"{preprocessed_data_folder}/{file_name}\", file_name)\n",
    "        except S3Error as err:\n",
    "            print(err)\n",
    "        file_list.append(file_name)\n",
    "        os.remove(file_name)\n",
    "        buffer = []\n",
    "    print(\"total time is :\", timeit.default_timer() - starttime)\n",
    "    return file_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85740989-eb5b-403b-939c-1e0e54ad1e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T05:24:41.753976Z",
     "iopub.status.busy": "2022-09-14T05:24:41.753453Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2022-09-14T05:24:41.642052",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process examples for prefix: train\n",
      "Total of 25000 elements\n",
      "Total of 50 files of 500 records - 0.003891207277774811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 1/50 - 69.78354430524632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 2/50 - 118.11841360805556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 3/50 - 162.00976834632456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 4/50 - 235.0743087893352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 5/50 - 279.11819615308195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 6/50 - 330.2023416701704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 7/50 - 361.97559532523155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 8/50 - 409.2729815039784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 9/50 - 456.77326090820134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 10/50 - 508.4792569312267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 11/50 - 568.6093635512516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 12/50 - 600.870526635088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 13/50 - 640.274657165166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 14/50 - 684.7047643181868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 15/50 - 717.8217672011815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with batch 16/50 - 773.6030188701116\n"
     ]
    }
   ],
   "source": [
    "process_examples(train, prefix=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb655b-de8d-428c-88a0-4c7cc9112747",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_examples(test, prefix=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813f0db-7925-4761-91b6-b4074cbb91d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(output_text_path, 'w') as writer:\n",
    "    writer.write(\"done!\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/hyper-scale-tensorflow-with-minio/pre-process.ipynb",
   "output_path": "/home/jovyan/hyper-scale-tensorflow-with-minio/pre-process.ipynb",
   "parameters": {},
   "start_time": "2022-09-14T05:22:52.049777",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}